from __future__ import absolute_import, division

import os
import math
import torch
import torch.nn             as nn
import torch.nn.functional  as F
import numpy                as np
import rnn_passes_cpp

from torch.autograd         import Function

class LSTM_Op_pass(Function):
    @staticmethod
    def forward(ctx, input, c_p, h_p, xOps, hOps):
        Xs, c, h = rnn_passes_cpp.LSTM_Op_forward(input.to(torch.double),
                                                  xOps.to(torch.double),
                                                  hOps.to(torch.double))

        output = Xs[0]
        ctx.save_for_backward(input, Xs, c, c_p, h_p, xOps, hOps)
        return output, c, h

    @staticmethod
    def backward(ctx, grad_output):
        input, Xs, c, c_p, h_p, xOps, hOps = ctx.saved_tensors
        grad_input, grad_xOps, grad_hOps = rnn_passes_cpp.LSTM_Op_backward(input.to(torch.double),
                                                                           Xs.to(torch.double),
                                                                           c.to(torch.double),
                                                                           c_p.to(torch.double),
                                                                           h_p.to(torch.double),
                                                                           xOps.to(torch.double),
                                                                           hOps.to(torch.double),
                                                                           grad_output.to(torch.double))
        return grad_input, grad_xOps, grad_hOps

# LSTM_Op is an LSTM layer whose kernels are true operators rather than Hadamard-like operators
# biases are not used
# multi-layer functionality is not currently supported
# bidirectionality is not currently supported

#todo: modify LSTM_Op to accept information regarding the time lapse between sequence elements
class LSTM_Op(nn.Module):
    def __init__(self, batch_size, input_dim, output_dim, hidden_size):
        super(LSTM_Op, self).__init__()
        self.input_dim = input_dim
        self.output_dim = output_dim
        self.hidden_size = hidden_size

        # operators act to the right -> Op*input
        # x operators map input sequences of length input_dim to sequences of length output_dim
        # h operators are square matrices of dimension hidden_size
        self.xOps = torch.zeros([4, self.output_dim, self.input_dim])
        self.hOps = torch.empty([4, self.hidden_size, self.hidden_size])
        self.hOps[:] = torch.eye(self.hidden_size)
        
        # set x operators to identity in input_dim-dimensional space
        for i in range(self.input_dim):
            self.xOps[:][i][i] = 1

        # cell and hidden states are both vectors of length hidden_size
        self.c = torch.zeros([self.batch_size, self.hidden_size])
        self.h = torch.zeros([self.batch_size, self.hidden_size])

        def forward(self, input):
            return LSTM_Op.apply(input, self.c, self.h, self.xOps, self.hOps)
